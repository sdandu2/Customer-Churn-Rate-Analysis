{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to kaggle.json file\n",
    "kaggle_json_path = os.path.expanduser(\"C:/Users/sammy/.kaggle/kaggle.json\")  # Adjust path as necessary\n",
    "\n",
    "# Ensure the Kaggle API can authenticate\n",
    "if not os.path.exists(kaggle_json_path):\n",
    "    raise FileNotFoundError(\"kaggle.json file not found at the specified path.\")\n",
    "\n",
    "# Set Kaggle API credentials using kaggle.json\n",
    "with open(kaggle_json_path, \"r\") as f:\n",
    "    credentials = json.load(f)\n",
    "os.environ['KAGGLE_USERNAME'] = credentials['username']\n",
    "os.environ['KAGGLE_KEY'] = credentials['key']\n",
    "\n",
    "# Authenticate and download the dataset\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Specify the dataset and download path\n",
    "dataset_id = \"blastchar/telco-customer-churn\"  # Dataset ID for Telco Customer Churn\n",
    "cache_dir = os.path.expanduser(\"C:/Users/sammy/OneDrive/Documents/Portfolio\")\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Download and unzip the dataset\n",
    "api.dataset_download_files(dataset_id, path=cache_dir, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: string (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|customerID|gender|SeniorCitizen|Partner|Dependents|tenure|PhoneService|   MultipleLines|InternetService|OnlineSecurity|OnlineBackup|DeviceProtection|TechSupport|StreamingTV|StreamingMovies|      Contract|PaperlessBilling|       PaymentMethod|MonthlyCharges|TotalCharges|Churn|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "|7590-VHVEG|Female|            0|    Yes|        No|     1|          No|No phone service|            DSL|            No|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|         29.85|       29.85|   No|\n",
      "|5575-GNVDE|  Male|            0|     No|        No|    34|         Yes|              No|            DSL|           Yes|          No|             Yes|         No|         No|             No|      One year|              No|        Mailed check|         56.95|      1889.5|   No|\n",
      "|3668-QPYBK|  Male|            0|     No|        No|     2|         Yes|              No|            DSL|           Yes|         Yes|              No|         No|         No|             No|Month-to-month|             Yes|        Mailed check|         53.85|      108.15|  Yes|\n",
      "|7795-CFOCW|  Male|            0|     No|        No|    45|          No|No phone service|            DSL|           Yes|          No|             Yes|        Yes|         No|             No|      One year|              No|Bank transfer (au...|          42.3|     1840.75|   No|\n",
      "|9237-HQITU|Female|            0|     No|        No|     2|         Yes|              No|    Fiber optic|            No|          No|              No|         No|         No|             No|Month-to-month|             Yes|    Electronic check|          70.7|      151.65|  Yes|\n",
      "+----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+--------------------+--------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset using Spark\n",
    "spark = SparkSession.builder.appName(\"EnhancedChurnPredictionPipeline\").getOrCreate()\n",
    "data_path = os.path.join(cache_dir, \"WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Verify data ingestion\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "\n",
    "# Add feature engineering step\n",
    "df = df.withColumn(\"TotalSpend\", df.tenure * df.MonthlyCharges)\n",
    "\n",
    "# Index categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in [\"gender\", \"Partner\", \"Dependents\", \"Churn\", \"Contract\", \"PaymentMethod\"]]\n",
    "\n",
    "# Assemble feature columns, including new and scaled features\n",
    "assembler = VectorAssembler(inputCols=[\"gender_index\", \"Partner_index\", \"Dependents_index\", \"tenure\", \"MonthlyCharges\", \"TotalSpend\", \"Contract_index\", \"PaymentMethod_index\"], outputCol=\"raw_features\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Define logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Churn_index\")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up cross-validation\n",
    "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(labelCol=\"Churn_index\"), numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, crossval])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC after tuning and scaling: 0.8235969501088418\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Fit pipeline on training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model using AUC\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn_index\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC after tuning and scaling: {roc_auc}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
